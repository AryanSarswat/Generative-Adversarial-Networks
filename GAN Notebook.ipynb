{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483b8798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T02:17:42.011455Z",
     "start_time": "2022-06-08T02:17:41.999357Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5cfda4",
   "metadata": {},
   "source": [
    "# Utility For Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4e6f00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T03:23:12.998386Z",
     "start_time": "2022-06-08T03:23:12.986417Z"
    }
   },
   "outputs": [],
   "source": [
    "def modelSummary(model, verbose=False):\n",
    "    \"\"\"Method provides a description of a model and its parameters\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to summarize\n",
    "        verbose (bool, optional): Describes the model with specification for each layers. Defaults to False.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(model)\n",
    "    \n",
    "    total_parameters = 0\n",
    "        \n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.size()[0]\n",
    "        total_parameters += num_params\n",
    "        if verbose:\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"\\tNumber of parameters: {num_params}\")\n",
    "            print(f\"\\tShape: {param.shape}\")\n",
    "    \n",
    "    if total_parameters > 1e5:\n",
    "        print(f\"Total number of parameters: {total_parameters/1e6:.2f}M\")\n",
    "    else:\n",
    "        print(f\"Total number of parameters: {total_parameters/1e3:.2f}K\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a62cceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T02:14:54.860828Z",
     "start_time": "2022-06-08T02:14:54.830319Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvolutionBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channel,\n",
    "                      out_channels,\n",
    "                      kernel_size,\n",
    "                      stride,\n",
    "                      padding,\n",
    "                      bias=False), nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Convolution2dTransposeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
    "                 padding):\n",
    "        super(Convolution2dTransposeBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels,\n",
    "                               out_channels,\n",
    "                               kernel_size,\n",
    "                               stride,\n",
    "                               padding,\n",
    "                               bias=False), nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244604eb",
   "metadata": {},
   "source": [
    "# Training Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a63062b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T03:24:02.946990Z",
     "start_time": "2022-06-08T03:24:02.923055Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(discriminator: nn.Module, generator: nn.Module, device: torch.device, train_dataloader: DataLoader, training_params: dict, metrics: dict):\n",
    "    \"\"\"Method to train a model for one epoch\n",
    "\n",
    "    Args:\n",
    "        discriminator (nn.Module): discriminator to be trained\n",
    "        generator (nn.Module) : generator to be trained\n",
    "        device (str): device to be trained on\n",
    "        train_dataloader (nn.data.DataLoader): Dataloader object to load batches of dataset\n",
    "        training_params (dict): Dictionary of training parameters containing \"batch_size\", \"loss_function\"\n",
    "                                \"optimizer\".\n",
    "        metrics (dict): Dictionary of functional methods that would compute the metric value\n",
    "\n",
    "    Returns:\n",
    "        run_results (dict): Dictionary of metrics computed for the epoch\n",
    "    \"\"\"\n",
    "    OPTIMIZER_DISC = training_params[\"optimizer_discriminator\"]\n",
    "    OPTIMIZER_GEN = training_params[\"optimizer_generator\"]\n",
    "    CRITERION = training_params['loss_function']\n",
    "\n",
    "    discriminator = discriminator.to(device)\n",
    "    generator = generator.to(device)\n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "\n",
    "    # Dictionary holding result of this epoch\n",
    "    run_results = dict()\n",
    "    for metric in metrics:\n",
    "        run_results[metric] = 0.0\n",
    "    run_results[\"loss_discriminator\"] = 0.0\n",
    "    run_results[\"loss_generator\"] = 0.0\n",
    "\n",
    "    # Iterate over batches\n",
    "    num_batches = 0\n",
    "    for x, target in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        num_batches += 1\n",
    "\n",
    "        # Move tensors to device\n",
    "        real = x.to(device)\n",
    "        noise = torch.randn(\n",
    "            training_params['batch_size'], training_params['noise_dims']).to(device)\n",
    "        fake = generator(noise)\n",
    "\n",
    "        # Train Discriminator\n",
    "        discriminator_real_output = discriminator(real).view(-1)\n",
    "        # Detach because we dont want to accumalate gradients in the generator\n",
    "        discriminator_fake_output = discriminator(fake.detach()).view(-1)\n",
    "\n",
    "        loss_discriminator_real = CRITERION(\n",
    "            discriminator_real_output, torch.ones_like(discriminator_real_output))\n",
    "        loss_discriminator_fake = CRITERION(\n",
    "            discriminator_fake_output, torch.zeros_like(discriminator_fake_output))\n",
    "\n",
    "        loss_discriminator = (loss_discriminator_fake +\n",
    "                              loss_discriminator_real) / 2\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "        loss_discriminator.backward()\n",
    "        OPTIMIZER_DISC.step()\n",
    "\n",
    "        # Train Generator\n",
    "        discriminator_fake = discriminator(fake).view(-1)\n",
    "        loss_generator = CRITERION(\n",
    "            discriminator_fake, torch.ones_like(discriminator_fake))\n",
    "\n",
    "        generator.zero_grad()\n",
    "        loss_generator.backward()\n",
    "        OPTIMIZER_GEN.step()\n",
    "\n",
    "        # Update metrics\n",
    "        run_results[\"loss_generator\"] += loss_generator.detach().item()\n",
    "        run_results['loss_discriminator'] += loss_discriminator.detach().item()\n",
    "\n",
    "        for key, func in metrics.items():\n",
    "            run_results[key] += func(output, input).detach().item()\n",
    "\n",
    "        # Clean up memory\n",
    "        del real\n",
    "        del noise\n",
    "        del fake\n",
    "        del loss_discriminator\n",
    "        del loss_discriminator_real\n",
    "        del loss_discriminator_fake\n",
    "        del discriminator_fake\n",
    "        del loss_generator\n",
    "\n",
    "    for key in run_results:\n",
    "        run_results[key] /= num_batches\n",
    "\n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e07c2dce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T03:24:00.379408Z",
     "start_time": "2022-06-08T03:24:00.354212Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(discriminator: nn.Module, generator: nn.Module, device: torch.device, validation_dataloader: DataLoader, training_params: dict, metrics: dict):\n",
    "    \"\"\"Method to evaluate a model for one epoch\n",
    "\n",
    "    Args:\n",
    "        discriminator (nn.Module): discriminator to be evaluate\n",
    "        generator (nn.Module) : generator to be evaluate\n",
    "        device (str): device to evaluate on\n",
    "        validation_dataloader (DataLoader): DataLoader for evaluation\n",
    "        training_params (dict): Dictionary of training parameters containing \"batch_size\", \"loss_function\"\n",
    "                                \"optimizer\".\n",
    "        metrics (dict): Dictionary of functional methods that would compute the metric value\n",
    "\n",
    "    Returns:\n",
    "        run_results (dict): Dictionary of metrics computed for the epoch\n",
    "    \"\"\"\n",
    "    discriminator = discriminator.to(device)\n",
    "    generator = generator.to(device)\n",
    "\n",
    "    # Dictionary holding result of this epoch\n",
    "    run_results = dict()\n",
    "    for metric in metrics:\n",
    "        run_results[metric] = 0.0\n",
    "    run_results[\"loss_discriminator\"] = 0.0\n",
    "    run_results[\"loss_generator\"] = 0.0\n",
    "\n",
    "    # Iterate over batches\n",
    "    with torch.no_grad():\n",
    "        discriminator.eval()\n",
    "        generator.eval()\n",
    "        num_batches = 0\n",
    "\n",
    "        for x, target in tqdm(validation_dataloader, desc='Validation'):\n",
    "            # Move tensors to device\n",
    "            real = x.to(device)\n",
    "            noise = torch.randn(\n",
    "                training_params['batch_size'], training_params['noise_dims']).to(device)\n",
    "            fake = generator(noise)\n",
    "\n",
    "            # Evaluate Discriminator\n",
    "            discriminator_real_output = discriminator(real).view(-1)\n",
    "            # Detach because we dont want to accumalate gradients in the generator\n",
    "            discriminator_fake_output = discriminator(fake.detach()).view(-1)\n",
    "\n",
    "            loss_discriminator_real = CRITERION(\n",
    "                discriminator_real_output, torch.ones_like(discriminator_real_output))\n",
    "            loss_discriminator_fake = CRITERION(\n",
    "                discriminator_fake_output, torch.zeros_like(discriminator_fake_output))\n",
    "\n",
    "            loss_discriminator = (\n",
    "                loss_discriminator_fake + loss_discriminator_real) / 2\n",
    "\n",
    "            # Evaluate Generator\n",
    "            discriminator_fake = discriminator(fake).view(-1)\n",
    "            loss_generator = CRITERION(\n",
    "                discriminator_fake, torch.ones_like(discriminator_fake))\n",
    "\n",
    "            # Update metrics\n",
    "            run_results[\"loss_generator\"] += loss_generator.detach().item()\n",
    "            run_results['loss_discriminator'] += loss_discriminator.detach().item()\n",
    "\n",
    "            for key, func in metrics.items():\n",
    "                run_results[key] += func(output, input).detach().item()\n",
    "\n",
    "            # Clean up memory\n",
    "            del real\n",
    "            del noise\n",
    "            del fake\n",
    "            del loss_discriminator\n",
    "            del loss_discriminator_real\n",
    "            del loss_discriminator_fake\n",
    "            del discriminator_fake\n",
    "            del loss_generator\n",
    "\n",
    "    for key in run_results:\n",
    "        run_results[key] /= num_batches\n",
    "\n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61c503b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T03:23:57.629309Z",
     "start_time": "2022-06-08T03:23:57.607371Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_plots(fixed_noise, model, device, epoch, training_params):\n",
    "    \"\"\"Function to save plots of the model\n",
    "\n",
    "    Args:\n",
    "        fixed_samples (torch.Tensor): Samples to be plotted\n",
    "        fixed_noise (torch.Tensor): Noise to be plotted\n",
    "        model (nn.Module): Model to be tested\n",
    "        epoch (int): Epoch number\n",
    "        SAVE_PATH (str): Path to save plots\n",
    "    \"\"\"\n",
    "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
    "    SAVE_PATH = training_params[\"save_path\"]\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        fixed_noise = fixed_noise.to(device)\n",
    "\n",
    "        generated_images = model.decoder(fixed_noise)\n",
    "\n",
    "        _, axs = plt.subplots(10, 10, figsize=(30, 20))\n",
    "        axs = axs.flatten()\n",
    "\n",
    "        for image, ax in zip(generated_images, axs):\n",
    "            ax.imshow(image.cpu().numpy().reshape(28, 28))\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.savefig(f\"{SAVE_PATH}/generated_images/epoch{epoch + 1}.png\")\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        # Clean up memory\n",
    "        del generated_images\n",
    "        del image\n",
    "        del _, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7998e7b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T03:09:38.675546Z",
     "start_time": "2022-06-08T03:09:38.650809Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_evaluate(discriminator: nn.Module, generator: nn.Module, device: torch.device, train_dataloader: DataLoader, validation_dataloader: DataLoader, training_params: dict, metrics: dict):\n",
    "    \"\"\"Function to train a model and provide statistics during training\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to be trained\n",
    "        device (torch.device): Device to be trained on\n",
    "        train_dataset (DataLoader): Dataset to be trained on\n",
    "        validation_dataset (DataLoader): Dataset to be evaluated on\n",
    "        training_params (dict): Dictionary of training parameters containing \"num_epochs\", \"batch_size\", \"loss_function\",\n",
    "                                                                             \"save_path\", \"optimizer\"\n",
    "        metrics (dict): Dictionary of functional methods that would compute the metric value\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
    "    SAVE_PATH = training_params[\"save_path\"]\n",
    "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
    "    PLOT_EVERY = training_params[\"plot_every\"]\n",
    "    SAVE_EVERY = training_params[\"save_every\"]\n",
    "    LATENT_DIMS = training_params[\"latent_dims\"]\n",
    "\n",
    "    # Initialize metrics\n",
    "    train_results = dict()\n",
    "    train_results['loss_generator'] = np.empty(1)\n",
    "    train_results['loss_discriminator'] = np.empty(1)\n",
    "    evaluation_results = dict()\n",
    "    evaluation_results['loss'] = np.empty(1)\n",
    "    evaluation_results['loss'] = np.empty(1)\n",
    "\n",
    "    for metric in metrics:\n",
    "        train_results[metric] = np.empty(1)\n",
    "        evaluation_results[metric] = np.empty(1)\n",
    "\n",
    "    batch = next(iter(validation_dataloader))\n",
    "    idxs = []\n",
    "    for i in range(SAMPLE_SIZE):\n",
    "        idx = torch.where(batch[1] == i)[0].squeeze()[0]\n",
    "        idxs.append(idx.item())\n",
    "\n",
    "    FIXED_SAMPLES = batch[0][idxs].to(device).detach()\n",
    "\n",
    "    FIXED_NOISE = torch.normal(0, 1, size=(\n",
    "        100, LATENT_DIMS), device=device).detach()\n",
    "\n",
    "    # Clean up\n",
    "    del idxs\n",
    "    del batch\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        print(f\"=========== Epoch {epoch+1}/{NUM_EPOCHS} ===========\")\n",
    "\n",
    "        # Train Model\n",
    "        diff_train = psutil.virtual_memory().percent\n",
    "        epoch_train_results = train_epoch(\n",
    "            model, device, train_dataloader, training_params, metrics)\n",
    "        diff_train = psutil.virtual_memory().percent - diff_train\n",
    "\n",
    "        # Evaluate Model\n",
    "        diff_eval = psutil.virtual_memory().percent\n",
    "        epoch_evaluation_results = evaluate_epoch(\n",
    "            model, device, validation_dataloader, training_params, metrics)\n",
    "        diff_eval = psutil.virtual_memory().percent - diff_eval\n",
    "\n",
    "        diff_metric = psutil.virtual_memory().percent\n",
    "        for metric in metrics:\n",
    "            np.append(train_results[metric], epoch_train_results[metric])\n",
    "            np.append(evaluation_results[metric],\n",
    "                      epoch_evaluation_results[metric])\n",
    "        diff_metric = psutil.virtual_memory().percent - diff_metric\n",
    "\n",
    "        # Print results of epoch\n",
    "        print(\n",
    "            f\"Completed Epoch {epoch+1}/{NUM_EPOCHS} in {(time.time() - start):.2f}s\")\n",
    "        print(f\"Train Generator Loss: {epoch_train_results['loss_generator']:.2f} \\t Train Discriminator Loss: {epoch_train_results['loss_discriminator']:.2f}\" +\n",
    "              f\"\\nValidation Generator Loss: {epoch_evaluation_results['loss_generator']:.2f} \\t Validation Discriminator Loss: {epoch_evaluation_results['loss_discriminator']:.2f}\")\n",
    "\n",
    "        diff_plot = psutil.virtual_memory().percent\n",
    "        # Plot results\n",
    "        if epoch % PLOT_EVERY == 0:\n",
    "            save_plots(FIXED_NOISE, generator, device, epoch, training_params)\n",
    "        diff_plot = psutil.virtual_memory().percent - diff_plot\n",
    "\n",
    "        print(f\"Items cleaned up: {gc.collect()}\")\n",
    "\n",
    "        print(f\"Memory used in Training: {diff_train:.2f}%\")\n",
    "        print(f\"Memory used in Evaluation: {diff_eval:.2f}%\")\n",
    "        print(f\"Memory used in Metrics: {diff_metric:.2f}%\")\n",
    "        print(f\"Memory used in Plotting: {diff_plot:.2f}%\")\n",
    "        # Save model\n",
    "        if epoch % SAVE_EVERY == 0 and epoch != 0:\n",
    "            SAVE = f\"{SAVE_PATH}_epoch{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), SAVE)\n",
    "\n",
    "    return train_results, evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a410500",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c2a55a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T02:14:55.205846Z",
     "start_time": "2022-06-08T02:14:55.181870Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels, features_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels,\n",
    "                               features_dim,\n",
    "                               kernel_size=4,\n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "\n",
    "        self.conv2 = ConvolutionBlock(features_dim,\n",
    "                                      features_dim * 2,\n",
    "                                      kernel_size=4,\n",
    "                                      stride=2,\n",
    "                                      padding=1)\n",
    "        self.conv3 = ConvolutionBlock(features_dim * 2,\n",
    "                                      features_dim * 4,\n",
    "                                      kernel_size=4,\n",
    "                                      stride=2,\n",
    "                                      padding=1)\n",
    "        self.conv4 = ConvolutionBlock(features_dim * 4,\n",
    "                                      features_dim * 8,\n",
    "                                      kernel_size=4,\n",
    "                                      stride=2,\n",
    "                                      padding=1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(features_dim * 8,\n",
    "                               1,\n",
    "                               kernel_size=4,\n",
    "                               stride=1,\n",
    "                               padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_channels, input_channels, features_gen_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv2d1 = Convolution2dTransposeBlock(noise_channels,\n",
    "                                                   features_gen_dim * 16,\n",
    "                                                   kernel_size=4,\n",
    "                                                   stride=1,\n",
    "                                                   padding=0)\n",
    "        self.conv2d2 = Convolution2dTransposeBlock(features_gen_dim * 16,\n",
    "                                                   features_gen_dim * 8,\n",
    "                                                   kernel_size=4,\n",
    "                                                   stride=2,\n",
    "                                                   padding=1)\n",
    "        self.conv2d3 = Convolution2dTransposeBlock(features_gen_dim * 8,\n",
    "                                                   features_gen_dim * 4,\n",
    "                                                   kernel_size=4,\n",
    "                                                   stride=2,\n",
    "                                                   padding=1)\n",
    "        self.conv2d4 = Convolution2dTransposeBlock(features_gen_dim * 4,\n",
    "                                                   features_gen_dim * 2,\n",
    "                                                   kernel_size=4,\n",
    "                                                   stride=2,\n",
    "                                                   padding=1)\n",
    "\n",
    "        self.conv2d5 = nn.ConvTranspose2d(features_gen_dim * 2,\n",
    "                                          input_channels,\n",
    "                                          kernel_size=4,\n",
    "                                          stride=2,\n",
    "                                          padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d1(x)\n",
    "        x = self.conv2d2(x)\n",
    "        x = self.conv2d3(x)\n",
    "        x = self.conv2d4(x)\n",
    "        x = torch.tanh(self.conv2d5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8a555",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11b2892a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T03:23:51.381147Z",
     "start_time": "2022-06-08T03:23:50.859284Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24448/106336569.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m train_dataset = DataLoader(torchvision.datasets.CelebA(\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\celeba.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\celeba.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mdownload_file_from_google_drive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mextract_archive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"img_align_celeba.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_file_from_google_drive\u001b[1;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[0;32m    239\u001b[0m                 \u001b[1;34mf\"and can only be overcome by trying again later.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             )\n\u001b[1;32m--> 241\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0m_save_response_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_chunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse_content_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "IMG_CHANNELS = 3\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "dataset_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(IMG_CHANNELS)], [\n",
    "                         0.5 for _ in range(IMG_CHANNELS)])\n",
    "])\n",
    "\n",
    "train_dataset = DataLoader(torchvision.datasets.CelebA(\n",
    "    root='./data',\n",
    "    split='train',\n",
    "    download=True,\n",
    "    transform=dataset_transforms),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True)\n",
    "\n",
    "validation_dataset = DataLoader(torchvision.datasets.CelebA(\n",
    "    root='./data',\n",
    "    split='valid',\n",
    "    download=True,\n",
    "    transform=dataset_transforms),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c660d199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T03:23:24.725307Z",
     "start_time": "2022-06-08T03:23:24.711345Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-4\n",
    "FEATURES_DISC_DIM = 1024\n",
    "FEATURES_GEN_DIM = 1024\n",
    "FEATURES_NOISE_CHANNELS = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b9eeaca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T03:23:41.679658Z",
     "start_time": "2022-06-08T03:23:24.926769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 45.06K\n",
      "Total number of parameters: 92.26K\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(input_channels=3, features_dim=FEATURES_DISC_DIM).to(device)\n",
    "generator = Generator(noise_channels=FEATURES_NOISE_CHANNELS, input_channels=3, features_gen_dim=FEATURES_GEN_DIM).to(device)\n",
    "    \n",
    "modelSummary(discriminator)\n",
    "modelSummary(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691185a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
