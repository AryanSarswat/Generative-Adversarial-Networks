{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483b8798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:38:50.906410Z",
     "start_time": "2022-06-08T06:38:49.945679Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import tqdm.notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0299273",
   "metadata": {},
   "source": [
    "# Initialize wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b30af442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:38:52.288591Z",
     "start_time": "2022-06-08T06:38:50.909422Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maryansarswat\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5cfda4",
   "metadata": {},
   "source": [
    "# Utility For Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d3e839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:38:52.304518Z",
     "start_time": "2022-06-08T06:38:52.290562Z"
    }
   },
   "outputs": [],
   "source": [
    "def modelSummary(model, verbose=False):\n",
    "    \"\"\"Method provides a description of a model and its parameters\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to summarize\n",
    "        verbose (bool, optional): Describes the model with specification for each layers. Defaults to False.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(model)\n",
    "\n",
    "    total_parameters = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.size()[0]\n",
    "        total_parameters += num_params\n",
    "        if verbose:\n",
    "            print(f\"Layer: {name}\")\n",
    "            print(f\"\\tNumber of parameters: {num_params}\")\n",
    "            print(f\"\\tShape: {param.shape}\")\n",
    "\n",
    "    if total_parameters > 1e5:\n",
    "        print(f\"Total number of parameters: {total_parameters/1e6:.2f}M\")\n",
    "    else:\n",
    "        print(f\"Total number of parameters: {total_parameters/1e3:.2f}K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a62cceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:38:52.319974Z",
     "start_time": "2022-06-08T06:38:52.306522Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvolutionBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channel,\n",
    "                      out_channels,\n",
    "                      kernel_size,\n",
    "                      stride,\n",
    "                      padding,\n",
    "                      bias=False), nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Convolution2dTransposeBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
    "                 padding):\n",
    "        super(Convolution2dTransposeBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels,\n",
    "                               out_channels,\n",
    "                               kernel_size,\n",
    "                               stride,\n",
    "                               padding,\n",
    "                               bias=False), nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031fb1f",
   "metadata": {},
   "source": [
    "# Training Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b00326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:38:52.335929Z",
     "start_time": "2022-06-08T06:38:52.320937Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(discriminator: nn.Module, generator: nn.Module,\n",
    "                device: torch.device, train_dataloader: DataLoader,\n",
    "                training_params: dict, metrics: dict):\n",
    "    \"\"\"Method to train a model for one epoch\n",
    "\n",
    "    Args:\n",
    "        discriminator (nn.Module): discriminator to be trained\n",
    "        generator (nn.Module) : generator to be trained\n",
    "        device (str): device to be trained on\n",
    "        train_dataloader (nn.data.DataLoader): Dataloader object to load batches of dataset\n",
    "        training_params (dict): Dictionary of training parameters containing \"batch_size\", \"loss_function\"\n",
    "                                \"optimizer\".\n",
    "        metrics (dict): Dictionary of functional methods that would compute the metric value\n",
    "\n",
    "    Returns:\n",
    "        run_results (dict): Dictionary of metrics computed for the epoch\n",
    "    \"\"\"\n",
    "    OPTIMIZER_DISC = training_params[\"optimizer_discriminator\"]\n",
    "    OPTIMIZER_GEN = training_params[\"optimizer_generator\"]\n",
    "    CRITERION = training_params['loss_function']\n",
    "\n",
    "    discriminator = discriminator.to(device)\n",
    "    generator = generator.to(device)\n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "\n",
    "    wandb.watch(discriminator, CRITERION, log='all', log_freq=5)\n",
    "    wandb.watch(generator, CRITERION, log='all', log_freq=5)\n",
    "\n",
    "    # Dictionary holding result of this epoch\n",
    "    run_results = dict()\n",
    "    for metric in metrics:\n",
    "        run_results[metric] = 0.0\n",
    "    run_results[\"loss_discriminator\"] = 0.0\n",
    "    run_results[\"loss_generator\"] = 0.0\n",
    "\n",
    "    # Iterate over batches\n",
    "    num_batches = 0\n",
    "    for x, target in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        num_batches += 1\n",
    "\n",
    "        # Move tensors to device\n",
    "        real = x.to(device)\n",
    "        noise = torch.randn(training_params['batch_size'],\n",
    "                            training_params['noise_dims']).to(device)\n",
    "        fake = generator(noise)\n",
    "\n",
    "        # Train Discriminator\n",
    "        discriminator_real_output = discriminator(real).view(-1)\n",
    "        # Detach because we dont want to accumalate gradients in the generator\n",
    "        discriminator_fake_output = discriminator(fake.detach()).view(-1)\n",
    "\n",
    "        loss_discriminator_real = CRITERION(\n",
    "            discriminator_real_output,\n",
    "            torch.ones_like(discriminator_real_output))\n",
    "        loss_discriminator_fake = CRITERION(\n",
    "            discriminator_fake_output,\n",
    "            torch.zeros_like(discriminator_fake_output))\n",
    "\n",
    "        loss_discriminator = (loss_discriminator_fake +\n",
    "                              loss_discriminator_real) / 2\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "        loss_discriminator.backward()\n",
    "        OPTIMIZER_DISC.step()\n",
    "\n",
    "        # Train Generator\n",
    "        discriminator_fake = discriminator(fake).view(-1)\n",
    "        loss_generator = CRITERION(discriminator_fake,\n",
    "                                   torch.ones_like(discriminator_fake))\n",
    "\n",
    "        generator.zero_grad()\n",
    "        loss_generator.backward()\n",
    "        OPTIMIZER_GEN.step()\n",
    "\n",
    "        # Update metrics\n",
    "        run_results[\"loss_generator\"] += loss_generator.detach().item()\n",
    "        run_results['loss_discriminator'] += loss_discriminator.detach().item()\n",
    "\n",
    "        for key, func in metrics.items():\n",
    "            run_results[key] += func(output, input).detach().item()\n",
    "\n",
    "        # Clean up memory\n",
    "        del real\n",
    "        del noise\n",
    "        del fake\n",
    "        del loss_discriminator\n",
    "        del loss_discriminator_real\n",
    "        del loss_discriminator_fake\n",
    "        del discriminator_fake\n",
    "        del loss_generator\n",
    "\n",
    "    for key in run_results:\n",
    "        run_results[key] /= num_batches\n",
    "\n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5895e53c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:38:52.350856Z",
     "start_time": "2022-06-08T06:38:52.336894Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(discriminator: nn.Module, generator: nn.Module,\n",
    "                   device: torch.device, validation_dataloader: DataLoader,\n",
    "                   training_params: dict, metrics: dict):\n",
    "    \"\"\"Method to evaluate a model for one epoch\n",
    "\n",
    "    Args:\n",
    "        discriminator (nn.Module): discriminator to be evaluate\n",
    "        generator (nn.Module) : generator to be evaluate\n",
    "        device (str): device to evaluate on\n",
    "        validation_dataloader (DataLoader): DataLoader for evaluation\n",
    "        training_params (dict): Dictionary of training parameters containing \"batch_size\", \"loss_function\"\n",
    "                                \"optimizer\".\n",
    "        metrics (dict): Dictionary of functional methods that would compute the metric value\n",
    "\n",
    "    Returns:\n",
    "        run_results (dict): Dictionary of metrics computed for the epoch\n",
    "    \"\"\"\n",
    "    discriminator = discriminator.to(device)\n",
    "    generator = generator.to(device)\n",
    "\n",
    "    # Dictionary holding result of this epoch\n",
    "    run_results = dict()\n",
    "    for metric in metrics:\n",
    "        run_results[metric] = 0.0\n",
    "    run_results[\"loss_discriminator\"] = 0.0\n",
    "    run_results[\"loss_generator\"] = 0.0\n",
    "\n",
    "    # Iterate over batches\n",
    "    with torch.no_grad():\n",
    "        discriminator.eval()\n",
    "        generator.eval()\n",
    "        num_batches = 0\n",
    "\n",
    "        for x, target in tqdm(validation_dataloader, desc='Validation'):\n",
    "            # Move tensors to device\n",
    "            real = x.to(device)\n",
    "            noise = torch.randn(training_params['batch_size'],\n",
    "                                training_params['noise_dims']).to(device)\n",
    "            fake = generator(noise)\n",
    "\n",
    "            # Evaluate Discriminator\n",
    "            discriminator_real_output = discriminator(real).view(-1)\n",
    "            # Detach because we dont want to accumalate gradients in the generator\n",
    "            discriminator_fake_output = discriminator(fake.detach()).view(-1)\n",
    "\n",
    "            loss_discriminator_real = CRITERION(\n",
    "                discriminator_real_output,\n",
    "                torch.ones_like(discriminator_real_output))\n",
    "            loss_discriminator_fake = CRITERION(\n",
    "                discriminator_fake_output,\n",
    "                torch.zeros_like(discriminator_fake_output))\n",
    "\n",
    "            loss_discriminator = (loss_discriminator_fake +\n",
    "                                  loss_discriminator_real) / 2\n",
    "\n",
    "            # Evaluate Generator\n",
    "            discriminator_fake = discriminator(fake).view(-1)\n",
    "            loss_generator = CRITERION(discriminator_fake,\n",
    "                                       torch.ones_like(discriminator_fake))\n",
    "\n",
    "            # Update metrics\n",
    "            run_results[\"loss_generator\"] += loss_generator.detach().item()\n",
    "            run_results['loss_discriminator'] += loss_discriminator.detach(\n",
    "            ).item()\n",
    "\n",
    "            for key, func in metrics.items():\n",
    "                run_results[key] += func(output, input).detach().item()\n",
    "\n",
    "            # Clean up memory\n",
    "            del real\n",
    "            del noise\n",
    "            del fake\n",
    "            del loss_discriminator\n",
    "            del loss_discriminator_real\n",
    "            del loss_discriminator_fake\n",
    "            del discriminator_fake\n",
    "            del loss_generator\n",
    "\n",
    "    for key in run_results:\n",
    "        run_results[key] /= num_batches\n",
    "\n",
    "    return run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e68ba52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:38:52.366982Z",
     "start_time": "2022-06-08T06:38:52.352852Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_plots(fixed_noise, model, device, epoch, training_params):\n",
    "    \"\"\"Function to save plots of the model\n",
    "\n",
    "    Args:\n",
    "        fixed_samples (torch.Tensor): Samples to be plotted\n",
    "        fixed_noise (torch.Tensor): Noise to be plotted\n",
    "        model (nn.Module): Model to be tested\n",
    "        epoch (int): Epoch number\n",
    "        SAVE_PATH (str): Path to save plots\n",
    "    \"\"\"\n",
    "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
    "    SAVE_PATH = training_params[\"save_path\"]\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        fixed_noise = fixed_noise.to(device)\n",
    "\n",
    "        generated_images = model.decoder(fixed_noise)\n",
    "\n",
    "        _, axs = plt.subplots(10, 10, figsize=(30, 20))\n",
    "        axs = axs.flatten()\n",
    "\n",
    "        for image, ax in zip(generated_images, axs):\n",
    "            ax.imshow(image.cpu().numpy().reshape(28, 28))\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.savefig(f\"{SAVE_PATH}/generated_images/epoch{epoch + 1}.png\")\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        # Clean up memory\n",
    "        del generated_images\n",
    "        del image\n",
    "        del _, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6370ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:40:11.999499Z",
     "start_time": "2022-06-08T06:40:11.981546Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_evaluate(discriminator: nn.Module, generator: nn.Module,\n",
    "                   device: torch.device, train_dataloader: DataLoader,\n",
    "                   validation_dataloader: DataLoader, training_params: dict,\n",
    "                   metrics: dict):\n",
    "    \"\"\"Function to train a model and provide statistics during training\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to be trained\n",
    "        device (torch.device): Device to be trained on\n",
    "        train_dataset (DataLoader): Dataset to be trained on\n",
    "        validation_dataset (DataLoader): Dataset to be evaluated on\n",
    "        training_params (dict): Dictionary of training parameters containing \"num_epochs\", \"batch_size\", \"loss_function\",\n",
    "                                                                             \"save_path\", \"optimizer\"\n",
    "        metrics (dict): Dictionary of functional methods that would compute the metric value\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
    "    SAVE_PATH = training_params[\"save_path\"]\n",
    "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
    "    PLOT_EVERY = training_params[\"plot_every\"]\n",
    "    SAVE_EVERY = training_params[\"save_every\"]\n",
    "\n",
    "    FIXED_NOISE = torch.normal(0,\n",
    "                               1,\n",
    "                               size=(100,\n",
    "                                     training_params['features_noise_dim'], 1,\n",
    "                                     1),\n",
    "                               device=device).detach()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        print(f\"=========== Epoch {epoch+1}/{NUM_EPOCHS} ===========\")\n",
    "\n",
    "        # Train Model\n",
    "        epoch_train_results = train_epoch(model, device, train_dataloader,\n",
    "                                          training_params, metrics)\n",
    "\n",
    "        # Evaluate Model\n",
    "        epoch_evaluation_results = evaluate_epoch(model, device,\n",
    "                                                  validation_dataloader,\n",
    "                                                  training_params, metrics)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"training_disc_loss\": epoch_train_results['loss_discriminator'],\n",
    "            \"training_gen_loss\": epoch_train_results['loss_generator'],\n",
    "            \"validation_disc_loss\": epoch_evaluation_results['loss_discriminator'],\n",
    "            \"validation_gen_loss\": epoch_train_results['loss_generator'],\n",
    "        }, step=epoch)\n",
    "\n",
    "        for metric in metrics:\n",
    "            np.append(train_results[metric], epoch_train_results[metric])\n",
    "            np.append(evaluation_results[metric],\n",
    "                      epoch_evaluation_results[metric])\n",
    "\n",
    "        diff_metric = psutil.virtual_memory().percent - diff_metric\n",
    "\n",
    "        # Print results of epoch\n",
    "        print(\n",
    "            f\"Completed Epoch {epoch+1}/{NUM_EPOCHS} in {(time.time() - start):.2f}s\"\n",
    "        )\n",
    "\n",
    "        diff_plot = psutil.virtual_memory().percent\n",
    "        # Plot results\n",
    "        if epoch % PLOT_EVERY == 0:\n",
    "            save_plots(FIXED_NOISE, generator, device, epoch, training_params)\n",
    "        diff_plot = psutil.virtual_memory().percent - diff_plot\n",
    "\n",
    "        print(f\"Items cleaned up: {gc.collect()}\")\n",
    "        # Save model\n",
    "        if epoch % SAVE_EVERY == 0 and epoch != 0:\n",
    "            SAVE = f\"{SAVE_PATH}_epoch{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), SAVE)\n",
    "\n",
    "    return train_results, evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a410500",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69c2a55a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:40:13.344981Z",
     "start_time": "2022-06-08T06:40:13.325035Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels, features_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels,\n",
    "                               features_dim,\n",
    "                               kernel_size=4,\n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "\n",
    "        self.conv2 = ConvolutionBlock(features_dim,\n",
    "                                      features_dim * 2,\n",
    "                                      kernel_size=4,\n",
    "                                      stride=2,\n",
    "                                      padding=1)\n",
    "        self.conv3 = ConvolutionBlock(features_dim * 2,\n",
    "                                      features_dim * 4,\n",
    "                                      kernel_size=4,\n",
    "                                      stride=2,\n",
    "                                      padding=1)\n",
    "        self.conv4 = ConvolutionBlock(features_dim * 4,\n",
    "                                      features_dim * 8,\n",
    "                                      kernel_size=4,\n",
    "                                      stride=2,\n",
    "                                      padding=1)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(features_dim * 8,\n",
    "                               1,\n",
    "                               kernel_size=4,\n",
    "                               stride=1,\n",
    "                               padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_channels, input_channels, features_gen_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv2d1 = Convolution2dTransposeBlock(noise_channels,\n",
    "                                                   features_gen_dim * 16,\n",
    "                                                   kernel_size=4,\n",
    "                                                   stride=1,\n",
    "                                                   padding=0)\n",
    "        self.conv2d2 = Convolution2dTransposeBlock(features_gen_dim * 16,\n",
    "                                                   features_gen_dim * 8,\n",
    "                                                   kernel_size=4,\n",
    "                                                   stride=2,\n",
    "                                                   padding=1)\n",
    "        self.conv2d3 = Convolution2dTransposeBlock(features_gen_dim * 8,\n",
    "                                                   features_gen_dim * 4,\n",
    "                                                   kernel_size=4,\n",
    "                                                   stride=2,\n",
    "                                                   padding=1)\n",
    "        self.conv2d4 = Convolution2dTransposeBlock(features_gen_dim * 4,\n",
    "                                                   features_gen_dim * 2,\n",
    "                                                   kernel_size=4,\n",
    "                                                   stride=2,\n",
    "                                                   padding=1)\n",
    "\n",
    "        self.conv2d5 = nn.ConvTranspose2d(features_gen_dim * 2,\n",
    "                                          input_channels,\n",
    "                                          kernel_size=4,\n",
    "                                          stride=2,\n",
    "                                          padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d1(x)\n",
    "        x = self.conv2d2(x)\n",
    "        x = self.conv2d3(x)\n",
    "        x = self.conv2d4(x)\n",
    "        x = torch.tanh(self.conv2d5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a462d2",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76403a24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:40:15.161838Z",
     "start_time": "2022-06-08T06:40:14.270288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:82: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Model Parameters\n",
    "FEATURES_DISC_DIM = 1024\n",
    "FEATURES_GEN_DIM = 1024\n",
    "FEATURES_NOISE_DIM = 100\n",
    "\n",
    "# Training Parameters\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 256\n",
    "SAVE_PATH = \"training\"\n",
    "SAMPLE_SIZE = 10\n",
    "PLOT_EVERY = 10\n",
    "SAVE_EVERY = 50\n",
    "\n",
    "# Misc\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f8fa3",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11b2892a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:40:16.032097Z",
     "start_time": "2022-06-08T06:40:15.257646Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29124/1050989400.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m train_dataset = DataLoader(torchvision.datasets.CelebA(\n\u001b[0m\u001b[0;32m     12\u001b[0m     root='./data', split='train', download=True, transform=dataset_transforms),\n\u001b[0;32m     13\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\celeba.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\celeba.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[0mdownload_file_from_google_drive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mextract_archive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"img_align_celeba.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_file_from_google_drive\u001b[1;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[0;32m    239\u001b[0m                 \u001b[1;34mf\"and can only be overcome by trying again later.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             )\n\u001b[1;32m--> 241\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0m_save_response_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_chunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse_content_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later."
     ]
    }
   ],
   "source": [
    "IMG_CHANNELS = 3\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "dataset_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(IMG_CHANNELS)],\n",
    "                         [0.5 for _ in range(IMG_CHANNELS)])\n",
    "])\n",
    "\n",
    "train_dataset = DataLoader(torchvision.datasets.CelebA(\n",
    "    root='./data', split='train', download=True, transform=dataset_transforms),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True)\n",
    "\n",
    "validation_dataset = DataLoader(torchvision.datasets.CelebA(\n",
    "    root='./data', split='valid', download=True, transform=dataset_transforms),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5734b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T06:38:52.387757Z",
     "start_time": "2022-06-08T06:38:52.387757Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator(input_channels=IMG_CHANNELS,\n",
    "                              features_dim=FEATURES_DISC_DIM).to(device)\n",
    "\n",
    "generator = Generator(noise_channels=FEATURES_NOISE_DIM,\n",
    "                      input_channels=IMG_CHANNELS,\n",
    "                      features_gen_dim=FEATURES_GEN_DIM).to(device)\n",
    "\n",
    "# modelSummary(discriminator)\n",
    "# modelSummary(generator)\n",
    " \n",
    "training_params = {\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'loss_function': F.binary_cross_entropy,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'optimizer_discriminator': torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE),\n",
    "    'optimizer_discriminator': torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE),\n",
    "    'save_path': SAVE_PATH,\n",
    "    'sample_size': SAMPLE_SIZE,\n",
    "    'plot_every': PLOT_EVERY,\n",
    "    'save_every': SAVE_EVERY,\n",
    "    'features_disc_dim': FEATURES_DISC_DIM,\n",
    "    'features_gen_dim': FEATURES_GEN_DIM,\n",
    "    'features_noise_dim': FEATURE_NOISE_DIM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229d28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fc000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
